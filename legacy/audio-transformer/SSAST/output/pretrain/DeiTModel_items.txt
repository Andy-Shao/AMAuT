training=True
_parameters=OrderedDict([('cls_token', Parameter containing:
tensor([[[ 0.0020, -0.0363,  0.0426,  0.0013,  0.0215, -0.0157,  0.0099,
           0.0299, -0.0198, -0.0199,  0.0379,  0.0036,  0.0121, -0.0050,
          -0.0217, -0.0080, -0.0121, -0.0056,  0.0096,  0.0231, -0.0090,
          -0.0024, -0.0074, -0.0244, -0.0181,  0.0422,  0.0089,  0.0064,
          -0.0344, -0.0167,  0.0200,  0.0145, -0.0022, -0.0019, -0.0032,
          -0.0254, -0.0023, -0.0121, -0.0259,  0.0216, -0.0175,  0.0181,
           0.0039,  0.0012,  0.0088, -0.0045, -0.0109, -0.0091,  0.0192,
           0.0056, -0.0388,  0.0073,  0.0706,  0.0144,  0.0069, -0.0132,
          -0.0232, -0.0085, -0.0384, -0.0079, -0.0010,  0.0123,  0.0289,
           0.0334,  0.0232, -0.0375,  0.0060,  0.0170,  0.0068, -0.0127,
          -0.0019,  0.0155,  0.0041, -0.0221,  0.0112, -0.0133, -0.0024,
          -0.0199, -0.0044,  0.0041,  0.0291, -0.0091, -0.0444, -0.0177,
          -0.0183, -0.0091,  0.0014, -0.0144, -0.0051,  0.0054, -0.0055,
          -0.0071,  0.0109, -0.0196, -0.0186,  0.0115, -0.0370, -0.0301,
           0.0106, -0.0191, -0.0076, -0.0156, -0.0031,  0.0051, -0.0119,
           0.0266,  0.0025,  0.0173,  0.0216,  0.0278, -0.0070, -0.0053,
           0.0226, -0.0152, -0.0102, -0.0077,  0.0056,  0.0159, -0.0134,
           0.0437,  0.0135, -0.0110, -0.0089,  0.0046,  0.0064, -0.0053,
           0.0357, -0.0209, -0.0184, -0.0025, -0.0180,  0.0267, -0.0011,
          -0.0050,  0.0288,  0.0235,  0.0070,  0.0371, -0.0299,  0.0036,
          -0.0012, -0.0052,  0.0112, -0.0132, -0.0274,  0.0197, -0.0247,
          -0.0003,  0.0080, -0.0075, -0.0161, -0.0165, -0.0165, -0.0270,
           0.0182, -0.0463, -0.0156,  0.0061, -0.0070, -0.0277,  0.0075,
          -0.0064, -0.0079, -0.0083,  0.0083,  0.0104,  0.0071, -0.0356,
          -0.0070,  0.0103, -0.0040, -0.0532, -0.0174,  0.0137, -0.0074,
           0.0006,  0.0121, -0.0174, -0.0273,  0.0449, -0.0046, -0.0088,
           0.0296, -0.0313, -0.0009,  0.0014,  0.0002,  0.0271, -0.0152,
          -0.0427,  0.0084, -0.0142,  0.0222,  0.0025,  0.0341,  0.0086,
           0.0341,  0.0142, -0.0202,  0.0183, -0.0151,  0.0084, -0.0231,
          -0.0012,  0.0078, -0.0047,  0.0143,  0.0121,  0.0425,  0.0145,
          -0.0286, -0.0119, -0.0183, -0.0083,  0.0085,  0.0003,  0.0546,
          -0.0210,  0.0154,  0.0296,  0.0122, -0.0047, -0.0014,  0.0388,
           0.0065,  0.0013, -0.0122,  0.0151, -0.0186,  0.0162, -0.0118,
           0.0243,  0.0022,  0.0083,  0.0077,  0.0132,  0.0105,  0.0197,
          -0.0178,  0.0199, -0.0432, -0.0258,  0.0242, -0.0090, -0.0094,
          -0.0206, -0.0131, -0.0021,  0.0083, -0.0113, -0.0058,  0.0196,
           0.0010, -0.0273, -0.0099, -0.0009,  0.0091,  0.0152,  0.0163,
           0.0249,  0.0460,  0.0023,  0.0264, -0.0197, -0.0338,  0.0386,
          -0.0185,  0.0096,  0.0226,  0.0085, -0.0010, -0.0181,  0.0012,
          -0.0053, -0.0166, -0.0034, -0.0134, -0.0031,  0.0145,  0.0183,
           0.0024, -0.0168,  0.0179, -0.0049,  0.0098, -0.0006, -0.0247,
           0.0156,  0.0282, -0.0107,  0.0171, -0.0227,  0.0194, -0.0414,
          -0.0078, -0.0103, -0.0107, -0.0254, -0.0127,  0.0166,  0.0082,
           0.0178, -0.0055, -0.0085, -0.0408, -0.0124, -0.0023, -0.0116,
           0.0181,  0.0003,  0.0180, -0.0310, -0.0077, -0.0167,  0.0030,
           0.0243,  0.0148,  0.0060, -0.0099,  0.0046, -0.0071, -0.0210,
          -0.0237,  0.0162, -0.0079,  0.0091,  0.0030, -0.0588,  0.0313,
          -0.0255, -0.0076,  0.0303, -0.0175, -0.0053, -0.0066, -0.0238,
           0.0032, -0.0035, -0.0367, -0.0044,  0.0266, -0.0278,  0.0104,
           0.0103,  0.0176,  0.0029,  0.0171,  0.0162,  0.0179, -0.0266,
           0.0285, -0.0068,  0.0265, -0.0490, -0.0068,  0.0011,  0.0327,
           0.0174,  0.0189, -0.0087,  0.0146, -0.0050, -0.0149, -0.0046,
          -0.0196, -0.0012, -0.0113, -0.0047, -0.0014,  0.0075,  0.0039,
           0.0165,  0.0270,  0.0131,  0.0072,  0.0086,  0.0089, -0.0147,
           0.0089,  0.0293,  0.0136, -0.0279, -0.0006, -0.0137, -0.0140,
           0.0220, -0.0310, -0.0127,  0.0105, -0.0064, -0.0052,  0.0233,
          -0.0198,  0.0042,  0.0145, -0.0096,  0.0155, -0.0012,  0.0010,
          -0.0006, -0.0503,  0.0008,  0.0201,  0.0078,  0.0318, -0.0056,
           0.0124, -0.0064,  0.0414,  0.0144, -0.0158, -0.0023,  0.0130,
          -0.0251, -0.0109, -0.0090,  0.0185, -0.0160,  0.0135,  0.0595,
           0.0306,  0.0073, -0.0035, -0.0079,  0.0102,  0.0256, -0.0109,
           0.0148, -0.0210, -0.0087,  0.0098,  0.0031,  0.0264, -0.0116,
           0.0156,  0.0168,  0.0003, -0.0014,  0.0037, -0.0053,  0.0325,
           0.0261,  0.0017, -0.0022,  0.0224, -0.0498,  0.0194,  0.0077,
           0.0103, -0.0014,  0.0045, -0.0101,  0.0303, -0.0064,  0.0204,
           0.0029,  0.0410,  0.0251,  0.0263, -0.0010, -0.0006, -0.0065,
          -0.0046,  0.0247,  0.0131, -0.0020,  0.0163, -0.0171, -0.0107,
           0.0104, -0.0175, -0.0086,  0.0029, -0.0318, -0.0183, -0.0148,
           0.0270, -0.0412,  0.0058, -0.0201, -0.0068, -0.0059, -0.0034,
          -0.0087, -0.0003, -0.0198,  0.0095, -0.0122, -0.0308,  0.0011,
          -0.0186, -0.0107, -0.0047, -0.0146, -0.0142, -0.0208,  0.0267,
           0.0146,  0.0183, -0.0065, -0.0062, -0.0036, -0.0112, -0.0025,
           0.0074, -0.0108,  0.0146,  0.0244,  0.0038,  0.0206, -0.0011,
           0.0134,  0.0059, -0.0179,  0.0391, -0.0068, -0.0126,  0.0300,
           0.0017, -0.0058,  0.0021, -0.0118,  0.0162,  0.0156, -0.0181,
          -0.0512,  0.0046, -0.0017, -0.0166, -0.0036,  0.0153,  0.0228,
          -0.0068, -0.0110,  0.0201, -0.0117, -0.0061, -0.0322,  0.0002,
           0.0066, -0.0334,  0.0036,  0.0102,  0.0122, -0.0145, -0.0146,
           0.0083,  0.0150, -0.0178,  0.0108, -0.0006,  0.0335,  0.0277,
          -0.0417,  0.0061, -0.0418, -0.0054,  0.0209,  0.0164,  0.0247,
          -0.0549, -0.0232, -0.0379,  0.0414, -0.0111, -0.0059, -0.0086,
          -0.0067,  0.0004, -0.0102,  0.0106,  0.0052, -0.0112, -0.0080,
           0.0024,  0.0063, -0.0156,  0.0178,  0.0087,  0.0076, -0.0481,
           0.0290,  0.0119,  0.0107, -0.0326,  0.0014,  0.0191, -0.0137,
           0.0020,  0.0089,  0.0060, -0.0385, -0.0155, -0.0221, -0.0020,
          -0.0080,  0.0239, -0.0396, -0.0056, -0.0164, -0.0207,  0.0235,
          -0.0077,  0.0091,  0.0010, -0.0077,  0.0130,  0.0064, -0.0371,
          -0.0082, -0.0102,  0.0171, -0.0054,  0.0229, -0.0298,  0.0083,
          -0.0458, -0.0090, -0.0187,  0.0093, -0.0181,  0.0159,  0.0290,
           0.0167, -0.0169, -0.0294,  0.0251, -0.0065, -0.0332, -0.0196,
          -0.0141,  0.0031, -0.0172,  0.0130, -0.0052, -0.0348,  0.0186,
          -0.0210,  0.0192, -0.0330, -0.0143,  0.0179,  0.0136,  0.0094,
          -0.0228,  0.0113, -0.0016,  0.0425, -0.0550, -0.0134, -0.0307,
          -0.0008, -0.0062,  0.0621, -0.0157,  0.0123,  0.0288,  0.0223,
          -0.0037,  0.0128,  0.0358, -0.0235,  0.0063,  0.0062,  0.0326,
          -0.0412, -0.0007, -0.0118,  0.0296,  0.0082,  0.0283, -0.0200,
          -0.0252,  0.0078, -0.0198, -0.0078,  0.0165,  0.0514, -0.0250,
          -0.0088,  0.0212, -0.0012,  0.0029,  0.0015,  0.0051, -0.0086,
           0.0010,  0.0331, -0.0243, -0.0041,  0.0170, -0.0025,  0.0008,
           0.0229, -0.0005, -0.0044, -0.0035,  0.0372, -0.0112, -0.0416,
          -0.0199, -0.0629,  0.0281, -0.0081, -0.0288, -0.0188,  0.0181,
           0.0070,  0.0056,  0.0065,  0.0189, -0.0472, -0.0007, -0.0192,
          -0.0066,  0.0279,  0.0106,  0.0107, -0.0100, -0.0323,  0.0055,
          -0.0258, -0.0027, -0.0286,  0.0116,  0.0108, -0.0213, -0.0205,
           0.0038, -0.0337, -0.0019,  0.0022, -0.0009, -0.0072,  0.0017,
           0.0206, -0.0300,  0.0072, -0.0076,  0.0244,  0.0165, -0.0110,
           0.0277, -0.0088,  0.0075, -0.0152,  0.0123, -0.0352,  0.0265,
           0.0144,  0.0003, -0.0294, -0.0093,  0.0279, -0.0185, -0.0415,
           0.0370,  0.0107,  0.0302, -0.0075, -0.0088,  0.0154,  0.0059,
           0.0251,  0.0106,  0.0057,  0.0233,  0.0288]]], requires_grad=True)), ('pos_embed', Parameter containing:
tensor([[[-0.0144,  0.0055, -0.0353,  ..., -0.0025, -0.0324, -0.0263],
         [-0.0170,  0.0028,  0.0177,  ...,  0.0236,  0.0207,  0.0037],
         [ 0.0114, -0.0076,  0.0152,  ...,  0.0141,  0.0200, -0.0205],
         ...,
         [-0.0258,  0.0364,  0.0420,  ...,  0.0165, -0.0292, -0.0065],
         [ 0.0181, -0.0004, -0.0021,  ...,  0.0052, -0.0411,  0.0472],
         [ 0.0056,  0.0414,  0.0126,  ...,  0.0057,  0.0113,  0.0066]]],
       requires_grad=True)), ('dist_token', Parameter containing:
tensor([[[-1.6516e-02, -2.0035e-02,  8.2061e-03, -1.9906e-02,  9.6632e-03,
           1.4033e-03,  7.6369e-03,  1.0440e-02, -7.1374e-03,  4.2033e-02,
          -1.5350e-03, -1.9752e-02,  1.7115e-03,  1.7165e-02,  5.2982e-03,
          -8.2788e-03, -6.3098e-03,  4.6980e-03, -3.5966e-02, -1.4281e-03,
           1.1484e-02,  3.4991e-02, -1.6751e-02, -1.7325e-02, -1.7910e-02,
          -5.6996e-03, -2.4324e-02, -1.4809e-02,  7.3238e-03,  1.1427e-02,
           1.6203e-02,  1.5511e-02,  2.1885e-02, -2.6712e-02,  2.0676e-03,
          -1.6299e-02, -1.3828e-02,  1.1045e-02,  1.4172e-04, -3.2231e-03,
          -1.0119e-02,  7.8558e-03, -2.0716e-02, -1.2896e-02, -9.5511e-04,
           3.6003e-02,  2.3121e-02, -1.9380e-02, -2.7065e-02, -5.0709e-03,
           9.6665e-03,  8.3662e-03,  1.2077e-02, -2.0546e-03, -2.5723e-02,
           1.3456e-02, -1.1359e-02,  6.9458e-03,  5.9155e-03,  1.0622e-02,
           1.3376e-03,  2.7129e-02,  6.3104e-03, -1.9356e-02,  1.6395e-02,
           2.3833e-02,  1.6781e-02, -9.1207e-03,  3.3622e-02, -5.1727e-03,
          -3.0978e-03,  1.9782e-02,  2.8515e-02, -2.9271e-02,  2.2542e-03,
           3.5390e-02, -1.8546e-02,  5.9896e-03, -1.1398e-02, -1.5649e-03,
           3.6439e-02,  9.9469e-03, -6.0531e-03,  1.6866e-02, -3.9496e-02,
           1.3837e-03, -2.8197e-02, -4.6907e-03,  3.7306e-03, -9.4773e-03,
           1.4788e-02,  1.2967e-02, -2.8380e-02, -1.3349e-02, -1.2368e-02,
           1.2606e-02, -2.7981e-02, -1.2866e-02,  1.9102e-02, -1.3136e-02,
          -1.0479e-03, -1.8858e-02, -2.7360e-02, -1.5513e-03, -1.6238e-02,
          -2.1298e-03, -6.5440e-03,  1.0265e-02,  1.5607e-02,  1.0573e-02,
           2.2101e-02,  2.1166e-02, -2.7758e-02, -1.1111e-02,  2.8726e-02,
           2.7004e-02, -1.4853e-02, -4.4335e-03, -3.2867e-02, -6.4495e-03,
           2.8262e-02,  5.6224e-03, -6.3485e-03,  1.8423e-02,  5.5930e-03,
           1.2526e-02, -2.3707e-02,  1.4806e-02,  1.3263e-03, -6.1193e-03,
          -1.0924e-02, -3.1293e-02,  1.3213e-02,  3.1213e-03,  1.8536e-02,
          -8.6123e-03, -1.8988e-02,  1.4357e-02, -1.2374e-02, -1.5263e-02,
           2.8878e-03, -5.8539e-03, -1.6506e-02, -5.5663e-03, -7.9605e-03,
          -9.4364e-03,  2.2012e-02, -2.9662e-03, -1.9449e-02, -1.0293e-02,
          -6.8736e-05,  6.3204e-03,  5.1885e-02,  1.4031e-02,  1.2604e-02,
          -2.0436e-02, -4.4673e-03, -2.6159e-03,  2.6888e-02, -3.9981e-04,
          -2.7490e-02,  1.4848e-02, -5.7852e-03, -1.8534e-03, -3.0144e-02,
          -5.6163e-03, -1.4450e-02, -1.8573e-02,  2.2704e-02, -2.4585e-02,
          -4.9428e-03,  3.1473e-03, -1.0941e-02,  6.7665e-03,  2.0806e-02,
          -1.4639e-02,  3.6722e-03,  7.1662e-03,  1.7742e-02,  5.7864e-03,
          -3.0906e-02,  2.8033e-02, -1.7795e-02, -5.9345e-03, -8.2975e-03,
           2.0812e-03, -6.9021e-03,  3.9000e-02, -3.1275e-02, -8.9221e-03,
           1.2564e-03, -1.2494e-02,  2.9268e-02,  5.4254e-03, -1.7059e-02,
           3.1983e-02, -1.6597e-02, -8.0321e-03,  3.7239e-03, -6.2885e-03,
          -2.4916e-03, -1.1076e-02, -1.8609e-03,  2.2103e-02, -8.3289e-03,
          -2.4703e-02, -3.6435e-02,  2.3359e-02, -4.0438e-02,  1.7873e-02,
          -9.4664e-06, -2.8798e-02,  3.6393e-03,  9.6205e-03,  4.0430e-03,
          -1.2108e-02, -3.9769e-03, -7.9324e-03, -4.4536e-03, -4.7895e-03,
          -3.1930e-03, -2.7322e-02,  1.2772e-02, -6.4956e-03, -4.5159e-03,
          -1.7114e-02, -1.4754e-02, -1.0910e-02,  2.0767e-02, -2.0133e-02,
           3.8610e-02,  2.3035e-02, -1.3300e-02, -2.4816e-02,  2.6000e-03,
          -4.2737e-02, -3.3097e-02, -5.3727e-03, -6.1000e-03, -2.3688e-02,
           2.2743e-02,  1.7660e-03,  2.0502e-02, -3.7315e-02, -5.0081e-04,
          -1.2296e-03, -7.2828e-03, -2.4152e-02, -2.8805e-04,  3.1139e-02,
           1.8548e-02, -1.1015e-02, -2.8904e-02, -3.7997e-02, -5.4709e-02,
          -3.1601e-02, -1.4766e-02,  7.5066e-03, -3.4389e-03,  7.0625e-03,
           1.7034e-02, -2.1442e-02,  3.8382e-03,  2.2322e-03,  3.5110e-02,
          -1.7106e-02,  6.3254e-03, -3.5621e-02, -8.2673e-03,  8.1506e-03,
          -5.4405e-03,  3.5544e-02,  4.5660e-02,  8.5861e-05, -1.0421e-02,
          -4.6565e-02,  4.0798e-02, -3.2301e-02,  4.1091e-02, -1.4128e-02,
           7.6643e-03, -6.9207e-03,  2.2771e-02,  7.3055e-04,  2.2322e-02,
           2.9027e-02,  2.2873e-02,  1.5083e-02, -6.2060e-03,  1.0427e-02,
           4.5293e-03, -1.8567e-02, -9.9491e-03, -1.2551e-02, -2.7367e-03,
          -8.6150e-03,  2.0182e-02,  1.9745e-02, -4.0011e-02,  1.3376e-02,
          -1.7838e-02,  2.5797e-02,  8.5338e-03,  5.1531e-02,  1.8626e-02,
           4.1323e-02,  2.2735e-02,  1.2821e-02, -3.2066e-03, -1.5248e-02,
           6.0727e-03, -3.2797e-03,  1.8514e-02,  2.2504e-02,  5.8425e-04,
          -4.8627e-03, -2.1258e-02,  2.4635e-02,  1.8739e-02,  9.0925e-03,
          -1.8228e-02, -9.1779e-03,  7.5867e-03,  1.4860e-02,  6.2958e-03,
          -1.0455e-02,  2.4292e-02,  1.2086e-02,  9.0054e-03, -9.1896e-03,
           4.8095e-03,  1.9135e-02, -8.1182e-03, -1.5268e-02, -3.0073e-02,
          -2.3787e-02, -1.9114e-03, -5.3698e-03,  7.2786e-03, -7.8059e-03,
          -1.7770e-02,  4.2569e-03, -4.7146e-02,  3.1118e-03,  2.5749e-03,
          -7.2709e-03, -5.0465e-02,  3.1653e-02, -3.3497e-03,  1.3192e-02,
           8.6782e-05, -4.5243e-02, -1.9380e-02, -2.8057e-02, -1.6231e-02,
          -1.9470e-03,  2.1541e-02,  2.1994e-02, -1.5592e-02,  2.5469e-02,
          -6.0389e-03,  4.1944e-02, -2.5726e-02, -1.8163e-02, -1.1186e-02,
          -2.1714e-02, -5.3287e-05,  1.1923e-02,  1.4242e-02, -2.1203e-02,
           3.8102e-02, -1.2046e-02,  3.2108e-03,  1.6005e-02, -1.2180e-03,
           2.8143e-03,  9.9920e-03, -5.2925e-03, -3.3517e-03,  2.7841e-02,
          -5.6473e-04, -7.2271e-03, -1.5540e-02, -5.1574e-03, -2.5310e-02,
           9.4121e-03, -1.5936e-04, -1.1352e-03, -1.1821e-02, -2.4924e-02,
           4.2807e-04,  4.5606e-03,  1.0141e-02,  8.9702e-03,  2.6006e-03,
          -8.8248e-03,  2.4837e-02, -1.9168e-02, -1.1494e-02, -4.7693e-03,
          -3.6988e-02,  1.0048e-02,  3.1505e-02,  4.4393e-03, -8.6887e-03,
           5.2613e-03, -3.9038e-02, -1.0428e-02,  1.7526e-02,  2.0893e-03,
           2.7586e-02,  3.1470e-02, -5.9576e-03, -1.8888e-02,  8.4395e-03,
           3.2623e-03,  2.4600e-02, -2.5617e-02, -1.3246e-02,  6.9134e-03,
          -2.2279e-02, -1.7549e-02,  9.9213e-03, -1.1447e-02,  7.9911e-04,
          -1.0478e-02, -5.9754e-03, -1.2246e-02,  1.5371e-02, -4.1237e-02,
           2.7056e-03,  1.1499e-02, -4.0160e-03,  1.8530e-02,  1.3215e-02,
           2.8738e-02, -1.2314e-02,  1.7880e-02,  1.6957e-03,  2.7983e-02,
          -2.6896e-02, -3.2500e-02,  1.9052e-02,  8.8228e-03, -2.3700e-02,
           2.6673e-02,  1.4406e-02,  2.6692e-02, -7.0195e-03, -1.2909e-02,
           9.3410e-03,  8.4832e-03, -3.8195e-02,  2.9411e-03, -8.6867e-03,
          -7.3945e-03, -8.6778e-03,  1.0648e-04,  1.7074e-02, -1.4634e-02,
          -7.2084e-03, -6.5180e-03,  1.1270e-02,  3.2636e-02, -1.0969e-02,
          -5.5772e-03, -1.6240e-02,  1.1049e-02,  3.1577e-02,  1.1591e-03,
          -4.4237e-03, -6.2428e-03, -3.6561e-03,  1.1281e-02,  5.4587e-03,
          -1.6320e-02,  1.4085e-02, -3.4243e-02, -4.0538e-03, -1.5346e-02,
          -3.5876e-02,  2.1440e-02,  1.0289e-02, -2.3114e-02,  1.1489e-02,
          -1.5013e-02,  4.7267e-02, -4.9438e-03,  6.2692e-04, -7.6847e-03,
          -3.7814e-02,  5.4164e-03,  4.2206e-03, -1.2343e-02, -2.2934e-02,
          -2.6591e-02, -5.0604e-04, -7.3652e-03,  2.4470e-02, -2.6250e-02,
          -3.6441e-02,  3.2518e-02,  1.7162e-02,  2.7230e-02,  4.3131e-02,
          -3.2152e-02, -9.4447e-03,  1.5776e-02, -2.8272e-02, -1.8034e-02,
          -3.7125e-03, -1.7978e-02, -1.2055e-02,  3.7630e-05, -6.7019e-03,
          -1.6063e-02,  3.9605e-03,  1.1174e-02,  1.0715e-03,  8.7712e-03,
           6.3211e-02, -3.1630e-02, -3.9769e-02, -4.4256e-02, -6.0846e-03,
          -4.1286e-02, -2.0050e-02,  6.3176e-03, -1.3511e-02,  1.7647e-02,
           2.4661e-02, -2.5131e-03, -2.3083e-03,  2.9003e-02,  1.9899e-02,
           1.7535e-02,  2.6742e-02, -9.3163e-03,  2.0843e-02,  8.0832e-03,
           1.9846e-02,  4.8394e-02, -6.1856e-02,  2.5172e-02,  2.3718e-02,
          -2.2815e-02,  3.1482e-02, -9.2883e-03,  2.4413e-02,  1.0242e-02,
          -2.5748e-02,  1.3319e-02,  2.4341e-02,  8.2040e-04, -1.8930e-02,
          -1.3189e-02,  3.2446e-02, -1.8129e-02,  4.3080e-03,  8.0381e-04,
          -1.0129e-02,  6.7907e-03, -1.3628e-02,  2.8797e-02, -7.9756e-03,
           2.5582e-02,  1.4236e-02, -8.3399e-03,  9.8728e-04,  1.9223e-02,
          -4.5604e-03,  4.7306e-03, -1.8443e-03,  2.9987e-02, -3.3385e-02,
          -4.3731e-03, -1.7222e-02,  4.6250e-03,  1.4154e-02,  2.4281e-02,
          -4.1466e-03,  7.4591e-03, -1.0519e-02, -4.3207e-02,  3.9041e-02,
          -2.7686e-02, -8.6742e-03, -2.1807e-02,  7.2841e-03, -4.0135e-03,
          -6.8546e-03, -1.9095e-02, -1.0590e-02,  1.1317e-02, -8.1322e-03,
          -3.0690e-02, -1.4113e-02, -2.5485e-02, -5.5065e-02, -3.5146e-02,
           1.8458e-02, -2.1230e-02, -1.5429e-02,  3.3658e-02,  4.2601e-02,
           3.5240e-03, -7.4813e-03, -2.9101e-02,  6.0292e-03, -3.1421e-03,
          -8.0582e-03,  9.5708e-03,  6.5404e-03,  9.7650e-03, -1.1956e-02,
           6.3496e-03,  5.4191e-03,  6.0938e-03,  8.1513e-03, -2.7245e-02,
          -1.2555e-02,  4.5009e-03, -2.5666e-03, -1.8165e-02,  5.2445e-03,
           4.3826e-03, -3.3747e-02,  1.0840e-02, -3.1020e-02,  2.4356e-02,
          -3.2746e-02, -1.0794e-02,  7.6268e-03, -3.3684e-02,  6.4892e-03,
          -1.3347e-02, -7.9232e-03,  2.7015e-02, -1.9430e-03,  1.6478e-02,
          -7.9586e-03, -1.9383e-03,  7.1795e-03,  9.3654e-03,  2.8243e-02,
           1.4190e-02, -4.0292e-02,  6.7856e-03, -1.4506e-03,  4.4709e-03,
          -2.9784e-02, -1.6402e-02,  1.1691e-02,  1.0269e-02, -7.7274e-03,
          -1.0594e-02,  5.0047e-03, -1.3240e-03,  2.2978e-03, -1.6106e-02,
          -1.4918e-02, -4.4788e-02, -1.1036e-02, -5.8384e-03,  2.9797e-02,
          -1.3452e-02,  1.3426e-02,  3.2008e-02,  2.3530e-02,  3.3411e-03,
           2.4940e-03, -3.2914e-02,  1.2430e-02,  7.3893e-03, -3.5518e-02,
          -2.5035e-02, -1.6478e-02,  3.2386e-02,  3.4723e-02, -3.5725e-03,
           1.8507e-03, -3.5041e-02, -2.9996e-02,  1.5930e-02,  2.0781e-02,
           1.9724e-03, -5.4183e-03, -1.4766e-02, -1.9670e-03,  6.1404e-02,
           1.0020e-02, -1.5204e-03,  1.9295e-02, -1.1727e-02,  8.9280e-03,
          -2.8200e-02, -4.5241e-03,  2.8080e-02, -2.7352e-03,  8.0344e-03,
           3.5156e-03,  1.5729e-02, -1.4400e-02,  4.2935e-02, -1.1430e-02,
           4.9319e-02,  1.4878e-02,  3.7805e-02,  1.0662e-02,  2.5180e-02,
           2.0409e-02, -3.4499e-04,  2.6017e-02,  1.3175e-02,  7.0075e-03,
           2.1155e-02,  2.1956e-03,  1.3592e-03, -1.2133e-02, -6.0917e-04,
           1.9134e-02,  1.7725e-02,  2.1872e-02, -2.3664e-03,  6.1116e-03,
           2.9524e-03, -1.0272e-02, -4.0548e-03,  3.3815e-02, -1.6775e-02,
          -2.1620e-02,  2.2987e-02,  2.0606e-02,  4.4920e-03,  7.5791e-03,
          -9.6765e-03,  2.6382e-02, -1.7430e-02,  2.9303e-02,  2.3173e-02,
           2.5901e-02, -1.2436e-02,  1.1825e-02, -1.8466e-02, -4.5362e-02,
          -2.5222e-03, -1.0568e-03, -1.7158e-02,  1.3193e-03,  3.5243e-03,
           5.6752e-03,  7.1458e-03,  1.6499e-02, -1.6412e-02, -2.8305e-02,
           1.5419e-02,  1.5268e-02,  3.6954e-03,  5.3851e-03, -1.3731e-02,
           2.7141e-02, -6.7820e-04,  1.4951e-02,  2.2812e-02,  3.1846e-02,
          -1.3034e-02, -1.3176e-02,  2.8853e-02]]], requires_grad=True))])
_buffers=OrderedDict()
_non_persistent_buffers_set=set()
_backward_hooks=OrderedDict()
_is_full_backward_hook=None
_forward_hooks=OrderedDict()
_forward_pre_hooks=OrderedDict()
_state_dict_hooks=OrderedDict()
_load_state_dict_pre_hooks=OrderedDict()
_modules=OrderedDict([('patch_embed', PatchEmbed(
  (proj): Conv2d(1, 768, kernel_size=(16, 16), stride=(16, 16))
)), ('pos_drop', Dropout(p=0.0, inplace=False)), ('blocks', ModuleList(
  (0): Block(
    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (attn): Attention(
      (qkv): Linear(in_features=768, out_features=2304, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=768, out_features=768, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (drop_path): Identity()
    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=768, out_features=3072, bias=True)
      (act): GELU()
      (fc2): Linear(in_features=3072, out_features=768, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
  )
  (1): Block(
    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (attn): Attention(
      (qkv): Linear(in_features=768, out_features=2304, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=768, out_features=768, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (drop_path): Identity()
    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=768, out_features=3072, bias=True)
      (act): GELU()
      (fc2): Linear(in_features=3072, out_features=768, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
  )
  (2): Block(
    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (attn): Attention(
      (qkv): Linear(in_features=768, out_features=2304, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=768, out_features=768, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (drop_path): Identity()
    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=768, out_features=3072, bias=True)
      (act): GELU()
      (fc2): Linear(in_features=3072, out_features=768, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
  )
  (3): Block(
    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (attn): Attention(
      (qkv): Linear(in_features=768, out_features=2304, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=768, out_features=768, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (drop_path): Identity()
    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=768, out_features=3072, bias=True)
      (act): GELU()
      (fc2): Linear(in_features=3072, out_features=768, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
  )
  (4): Block(
    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (attn): Attention(
      (qkv): Linear(in_features=768, out_features=2304, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=768, out_features=768, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (drop_path): Identity()
    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=768, out_features=3072, bias=True)
      (act): GELU()
      (fc2): Linear(in_features=3072, out_features=768, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
  )
  (5): Block(
    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (attn): Attention(
      (qkv): Linear(in_features=768, out_features=2304, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=768, out_features=768, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (drop_path): Identity()
    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=768, out_features=3072, bias=True)
      (act): GELU()
      (fc2): Linear(in_features=3072, out_features=768, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
  )
  (6): Block(
    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (attn): Attention(
      (qkv): Linear(in_features=768, out_features=2304, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=768, out_features=768, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (drop_path): Identity()
    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=768, out_features=3072, bias=True)
      (act): GELU()
      (fc2): Linear(in_features=3072, out_features=768, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
  )
  (7): Block(
    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (attn): Attention(
      (qkv): Linear(in_features=768, out_features=2304, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=768, out_features=768, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (drop_path): Identity()
    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=768, out_features=3072, bias=True)
      (act): GELU()
      (fc2): Linear(in_features=3072, out_features=768, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
  )
  (8): Block(
    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (attn): Attention(
      (qkv): Linear(in_features=768, out_features=2304, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=768, out_features=768, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (drop_path): Identity()
    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=768, out_features=3072, bias=True)
      (act): GELU()
      (fc2): Linear(in_features=3072, out_features=768, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
  )
  (9): Block(
    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (attn): Attention(
      (qkv): Linear(in_features=768, out_features=2304, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=768, out_features=768, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (drop_path): Identity()
    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=768, out_features=3072, bias=True)
      (act): GELU()
      (fc2): Linear(in_features=3072, out_features=768, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
  )
  (10): Block(
    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (attn): Attention(
      (qkv): Linear(in_features=768, out_features=2304, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=768, out_features=768, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (drop_path): Identity()
    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=768, out_features=3072, bias=True)
      (act): GELU()
      (fc2): Linear(in_features=3072, out_features=768, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
  )
  (11): Block(
    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (attn): Attention(
      (qkv): Linear(in_features=768, out_features=2304, bias=True)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=768, out_features=768, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (drop_path): Identity()
    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (mlp): Mlp(
      (fc1): Linear(in_features=768, out_features=3072, bias=True)
      (act): GELU()
      (fc2): Linear(in_features=3072, out_features=768, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
  )
)), ('norm', LayerNorm((768,), eps=1e-06, elementwise_affine=True)), ('pre_logits', Identity()), ('head', Linear(in_features=768, out_features=1000, bias=True)), ('head_dist', Linear(in_features=768, out_features=1000, bias=True))])
num_classes=1000
num_features=768
embed_dim=768
default_cfg={'url': 'https://dl.fbaipublicfiles.com/deit/deit_base_distilled_patch16_384-d0272ac0.pth', 'num_classes': 1000, 'input_size': (3, 384, 384), 'pool_size': None, 'crop_pct': 1.0, 'interpolation': 'bicubic', 'mean': (0.485, 0.456, 0.406), 'std': (0.229, 0.224, 0.225), 'first_conv': 'patch_embed.proj', 'classifier': 'head'}
